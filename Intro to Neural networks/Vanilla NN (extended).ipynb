{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39f4eb55",
   "metadata": {},
   "source": [
    "## Vanilla neural network \n",
    "### 3 Hidden layers\n",
    "This is a very basic code that which explains the basic functioning/concepts of neural networks \n",
    "In this neural network we were able to reduce the loss from a factor of `1e+4` to `<10` <br>\n",
    "Increasing number of epochs could lead to overfitting and increase the value of loss calculated<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61c91993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "import numpy as np\n",
    "from numpy.random import randn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632a6833",
   "metadata": {},
   "source": [
    "Initializing weights and data<br>\n",
    "* N - Input (here we can imagine N as total number of input images inserted into a model)\n",
    "* Din - (Input.size) of each image inserted into neural network\n",
    "* H1, H2, H3 - Total number of neurons in the hidden layer1,2 and 3\n",
    "* Dout - Final Outputs (categories in which each image would be classified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8013e80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, Din, H1, H2, H3, Dout = 64, 10000, 100, 75, 50, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28fe243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We randomnly initialize input pixels for all images in x and vector representation of each class predicted by our neural network  \n",
    "x, y = randn(N, Din), randn(N, Dout)\n",
    "# dim(x) = (64,10000); dim(y) = (64,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1821de4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialized weights in between input layer and hidden layer in w1 and hidden layer and output layer in w2\n",
    "w1, w2, w3, w4 = randn(Din, H1), randn(H1, H2), randn(H2, H3), randn(H3, Dout) \n",
    "# dim(w1) = (10000,100); dim(w2) = (100,50); dim(w3) = (50, 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2eeb29",
   "metadata": {},
   "source": [
    "Total epochs for which we will run our model to update weights in each iteration\n",
    "Epoch - iteration in which neural network completes one complete cycle of forward and backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f6b2f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17845.17828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-2490349c0ccc>:5: RuntimeWarning: overflow encountered in exp\n",
      "  h1 = 1.0/(1.0+np.exp(-x.dot(w1)))\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10000):\n",
    "# Forward propagation\n",
    "    # Using sigmoid activation function as a non-linear function to calculate the output from neurons in hidden layers\n",
    "    # dim(h1) = (64,100)\n",
    "    h1 = 1.0/(1.0+np.exp(-x.dot(w1)))\n",
    "    \n",
    "    #dim(h2) = (64,75)\n",
    "    h2 = 1.0/(1.0+np.exp(-h1.dot(w2)))\n",
    "    \n",
    "    #dim(h3) = (64,50)\n",
    "    h3 = 1.0/(1.0+np.exp(-h2.dot(w3)))\n",
    "    \n",
    "    # Predicting classes based on inputs and activation function used in our network\n",
    "    # dim(y_pred) = (64,10)\n",
    "    y_pred = h3.dot(w4)\n",
    "\n",
    "# Backward propagation\n",
    "# Compute loss (sigmoid activation, L2 (Euclidean) loss)\n",
    "    # Calculating the total loss after each iteration [sum(deviation in predicted output from true label)]\n",
    "    loss = round(np.square(y_pred-y).sum(),5)\n",
    "    if epoch == 0:\n",
    "        print(loss)\n",
    "    \n",
    "# Computing gradients\n",
    "    # Multiplying the loss vector for each input by a scalar so that it can be used more effectively to minimize loss \n",
    "    # (lower magnitude of predicetd loss would lead us to run more number of iterations)\n",
    "    # dim(dy_pred) = (64,10)\n",
    "    dy_pred = 2.0 * (y_pred-y)\n",
    "    \n",
    "    # Formluae for calculating [loss gradient = Loss * Output from previous layer]\n",
    "  \n",
    "    # Updating the change in weights required in a neural network\n",
    "    # dim(dw4) = (50,10); dim(h3.T) = (50,64); dim(dy_pred) = (64,10) \n",
    "    dw4 = h3.T.dot(dy_pred)\n",
    "    \n",
    "    # dim(dh3) = (64,50); dim(dy_pred) = (64,10); dim(w3.T) = (10,50)\n",
    "    dh3 = dy_pred.dot(w4.T)\n",
    "    dw3 = h2.T.dot(dh3*h3*(1-h3))\n",
    "    \n",
    "    # dim(dh2) = (64,50); dim(dy_pred) = (64,10); dim(w3.T) = (10,50)\n",
    "    dh2 = dh3.dot(w3.T)\n",
    "    dw2 = h1.T.dot(dh2*h2*(1-h2))\n",
    "    \n",
    "    # dim(dh1) = (64,50); dim(dy_pred) = (64,10); dim(w3.T) = (10,50)\n",
    "    dh1 = dh2.dot(w2.T)\n",
    "    dw1 = x.T.dot(dh1*h1*(1-h1))\n",
    "    \n",
    "# Stochastic Gradient (SGD) Step \n",
    "    # Updating the initialised weights (w1 & w2) for next iteration (using a learning rate parmeter which helps in \n",
    "    # overcoming the problem of vanishing gradient problem), dim(w1) & dim(w2) will remain same throughout execution\n",
    "    w1 -= 1e-4 * dw1\n",
    "    w2 -= 1e-4 * dw2\n",
    "    w3 -= 1e-4 * dw3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a19abba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9859"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e0158e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
