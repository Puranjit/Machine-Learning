{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39f4eb55",
   "metadata": {},
   "source": [
    "## Vanilla neural network\n",
    "This is a very basic code that which explains the basic functioning/concepts of neural networks \n",
    "In this neural network we were able to reduce the loss from a factor of 1e+4 to 1e-3 factor<br>\n",
    "Increasing number of epochs could lead to overfitting and increase the value of loss calculated<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "61c91993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "import numpy as np\n",
    "from numpy.random import randn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632a6833",
   "metadata": {},
   "source": [
    "Initializing weights and data<br>\n",
    "N - Input (here we can imagine N as total number of input images inserted into a model)<br>\n",
    "Din - Input.size<br>\n",
    "H - Total number of neurons in the hidden layer<br>\n",
    "Dout - Final Outputs (categories in which each image would be classified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8013e80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, Din, H, Dout = 64, 10000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "28fe243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = randn(N, Din), randn(N, Dout)\n",
    "# dim(x) = (64,10000); dim(y) = (64,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1821de4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialized weights in between input layer and hidden layer in w1 and hidden layer and output layer in w2\n",
    "w1, w2 = randn(Din, H), randn(H, Dout)\n",
    "# dim(w1) = (10000,100); dim(w2) = (100,10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2eeb29",
   "metadata": {},
   "source": [
    "Total epochs for which we will run our model to update weights in each iteration\n",
    "Epoch - iteration in which neural network completes one complete cycle of forward and backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1f6b2f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34205.54081\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10000):\n",
    "# Forward propagation\n",
    "    # Using sigmoid activation function (1/(1+e^(-x))) as a non-linear function to calculate the output from neurons in hidden layers\n",
    "    # dim(h) = (64,100)\n",
    "    h = 1.0/(1.0+np.exp(-x.dot(w1)))\n",
    "    \n",
    "    # Predicting classes based on inputs and activation function used in our network\n",
    "    # dim(y_pred) = (64,10)\n",
    "    y_pred = h.dot(w2)\n",
    "\n",
    "# Backward propagation\n",
    "# Compute loss (L2 (Euclidean) loss)\n",
    "    # Calculating the total loss after each iteration [sum((deviation in predicted output from true label)^2)]\n",
    "    loss = round(np.square(y_pred-y).sum(),7)\n",
    "    if epoch == 0:\n",
    "        print(loss)\n",
    "\n",
    "# Computing gradients\n",
    "    # Multiplying the loss vector for each input by a scalar so that it can be used more effectively to minimize loss \n",
    "    # (lower magnitude of predicetd loss would lead us to run more number of iterations)\n",
    "    # dim(dy_pred) = (64,10)\n",
    "    dy_pred = 2.0*(y_pred-y)\n",
    "\n",
    "    # Updating the w2 weights in neural network\n",
    "    # dim(h.T) = (100,64); dim(dy_pred) = (64,10) : dim(dw2) = (100,10)\n",
    "    dw2 = h.T.dot(dy_pred)\n",
    "\n",
    "    # Updating the weights w1 in neural network\n",
    "    dh = dy_pred.dot(w2.T)\n",
    "    dw1 = x.T.dot(dh*h*(1-h))\n",
    "\n",
    "# Stochastic Gradient (SGD) Step \n",
    "    # Updating the initialised weights (w1 & w2) for next iteration (using a learning rate parmeter which helps in \n",
    "    # overcoming the problem of vanishing gradient problem), dim(w1) & dim(w2) will remain same throughout execution\n",
    "    w1 -= 1e-4*dw1\n",
    "    w2 -= 1e-4*dw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a19abba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0272384"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542b51eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
